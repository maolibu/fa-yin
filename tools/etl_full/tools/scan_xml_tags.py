"""
CBETA XML æ ‡ç­¾æ‰«æå™¨
éå†æ‰€æœ‰ XML æ–‡ä»¶ï¼Œç»Ÿè®¡æ‰€æœ‰å‡ºç°çš„æ ‡ç­¾ã€å±æ€§ã€å‘½åç©ºé—´ï¼Œ
ç”¨äºéªŒè¯ cbeta_README.md çš„æ ‡ç­¾æ–‡æ¡£æ˜¯å¦å®Œæ•´ã€‚

è¾“å‡ºï¼š
1. æ‰€æœ‰å”¯ä¸€æ ‡ç­¾åŠå…¶å‡ºç°æ¬¡æ•°
2. æ¯ä¸ªæ ‡ç­¾çš„å±æ€§åˆ†å¸ƒ
3. æœªåœ¨ README ä¸­è®°å½•çš„æ ‡ç­¾
"""

import xml.etree.ElementTree as ET
import json
import os
import re
import sys
from collections import Counter, defaultdict
from pathlib import Path

DATAETL_DIR = Path(__file__).resolve().parent.parent
PROJECT_ROOT = DATAETL_DIR.parent
XML_BASE = PROJECT_ROOT / "01_data_raw" / "cbeta_xml_p5"

# README å·²è®°å½•çš„æ ‡ç­¾ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
README_TAGS = {
    # TEI header
    "TEI", "teiHeader", "fileDesc", "titleStmt", "title", "author",
    "publicationStmt", "idno", "date", "extent",
    "encodingDesc", "charDecl", "char", "charName", "charProp",
    "localName", "value", "mapping",
    # æ­£æ–‡
    "text", "body", "back",
    "lb", "pb", "p", "head",
    "div", "mulu", "juan", "jhead",
    "lg", "l", "caesura",
    "note", "app", "lem", "rdg",
    "anchor", "g", "byline", "ref",
    "space", "milestone",
}


def scan_file(xml_path, tag_counter, attr_counter, ns_set, sample_attrs):
    """æ‰«æå•ä¸ª XML æ–‡ä»¶ï¼Œæ”¶é›†æ ‡ç­¾å’Œå±æ€§ç»Ÿè®¡"""
    try:
        # ä½¿ç”¨ fromstring æ›¿ä»£ parseï¼Œé¿å… lxml/ET å°è¯•è§£æè¿œç¨‹ RNG schema è€ŒæŒ‚èµ·
        with open(str(xml_path), "r", encoding="utf-8") as f:
            content = f.read()
        tree = ET.ElementTree(ET.fromstring(content))
    except ET.ParseError as e:
        print(f"  âš ï¸ è§£æå¤±è´¥: {xml_path}: {e}", file=sys.stderr)
        return

    for elem in tree.iter():
        # å®Œæ•´æ ‡ç­¾åï¼ˆå«å‘½åç©ºé—´ï¼‰
        full_tag = elem.tag

        # æå–å‘½åç©ºé—´å’Œæœ¬åœ°å
        ns_match = re.match(r"\{(.+?)\}(.+)", full_tag)
        if ns_match:
            ns = ns_match.group(1)
            local = ns_match.group(2)
            ns_set.add(ns)
        else:
            local = full_tag

        tag_counter[local] += 1

        # æ”¶é›†å±æ€§
        for attr_name in elem.attrib:
            # å»å‘½åç©ºé—´
            attr_match = re.match(r"\{.+?\}(.+)", attr_name)
            attr_local = attr_match.group(1) if attr_match else attr_name
            attr_counter[local][attr_local] += 1

            # æ”¶é›†æ ·æœ¬å±æ€§å€¼ï¼ˆæ¯ä¸ªæœ€å¤š5ä¸ªï¼‰
            key = f"{local}@{attr_local}"
            if len(sample_attrs[key]) < 5:
                val = elem.get(attr_name, "")
                if val and val not in sample_attrs[key]:
                    sample_attrs[key].add(val)


def main():
    tag_counter = Counter()
    attr_counter = defaultdict(Counter)  # tag -> {attr: count}
    ns_set = set()
    sample_attrs = defaultdict(set)  # "tag@attr" -> {val1, val2, ...}

    # æ‰¾åˆ°æ‰€æœ‰ XML æ–‡ä»¶
    xml_files = sorted(XML_BASE.rglob("*.xml"))
    print(f"ğŸ“‚ æ‰¾åˆ° {len(xml_files)} ä¸ª XML æ–‡ä»¶")

    # æ‰«ææ¯ä¸ªæ–‡ä»¶
    for i, f in enumerate(xml_files):
        if i % 500 == 0:
            print(f"  æ‰«æä¸­... {i}/{len(xml_files)}")
        scan_file(f, tag_counter, attr_counter, ns_set, sample_attrs)

    print(f"\n{'='*70}")
    print(f"ğŸ“Š æ‰«æå®Œæˆ: {len(xml_files)} æ–‡ä»¶, {len(tag_counter)} ç§æ ‡ç­¾")
    print(f"{'='*70}")

    # 1. å‘½åç©ºé—´
    print(f"\n## å‘½åç©ºé—´ ({len(ns_set)})")
    for ns in sorted(ns_set):
        print(f"  - {ns}")

    # 2. æ‰€æœ‰æ ‡ç­¾ï¼ˆæŒ‰å‡ºç°æ¬¡æ•°æ’åºï¼‰
    print(f"\n## æ‰€æœ‰æ ‡ç­¾ ({len(tag_counter)})")
    print(f"{'æ ‡ç­¾':<25} {'æ¬¡æ•°':>10}  {'å±æ€§'}")
    print("-" * 70)
    for tag, count in tag_counter.most_common():
        attrs = dict(attr_counter[tag])
        attr_str = ", ".join(
            f"{a}({c})" for a, c in sorted(attrs.items(), key=lambda x: -x[1])[:5]
        )
        in_readme = "âœ…" if tag in README_TAGS else "âŒ"
        print(f"  {in_readme} {tag:<22} {count:>10,}  {attr_str}")

    # 3. README ä¸­æœ‰ä½†æ‰«æä¸­æ²¡æœ‰çš„æ ‡ç­¾
    scanned_tags = set(tag_counter.keys())
    readme_only = README_TAGS - scanned_tags
    if readme_only:
        print(f"\n## README æœ‰ä½†æœªæ‰«æåˆ°çš„æ ‡ç­¾ ({len(readme_only)})")
        for t in sorted(readme_only):
            print(f"  âš ï¸ {t}")

    # 4. æ‰«æåˆ°ä½† README æ²¡æœ‰çš„æ ‡ç­¾
    scan_only = scanned_tags - README_TAGS
    if scan_only:
        print(f"\n## æ‰«æåˆ°ä½† README æœªè®°å½•çš„æ ‡ç­¾ ({len(scan_only)})")
        for t in sorted(scan_only):
            count = tag_counter[t]
            attrs = dict(attr_counter[t])
            attr_str = ", ".join(
                f"{a}({c})" for a, c in sorted(attrs.items(), key=lambda x: -x[1])[:3]
            )
            # æ ·æœ¬å±æ€§å€¼
            samples = []
            for a in list(attrs.keys())[:3]:
                key = f"{t}@{a}"
                if sample_attrs[key]:
                    samples.append(f"{a}={list(sample_attrs[key])[:3]}")
            sample_str = " | ".join(samples) if samples else ""
            print(f"  ğŸ†• {t:<22} {count:>10,}  attrs: {attr_str}")
            if sample_str:
                print(f"     æ ·æœ¬: {sample_str}")

    # 5. ä¿å­˜ JSON æŠ¥å‘Š
    report = {
        "file_count": len(xml_files),
        "tag_count": len(tag_counter),
        "namespaces": sorted(ns_set),
        "tags": {
            tag: {
                "count": count,
                "in_readme": tag in README_TAGS,
                "attributes": dict(attr_counter[tag]),
            }
            for tag, count in tag_counter.most_common()
        },
    }
    report_path = DATAETL_DIR / "output" / "tag_scan_report.json"
    os.makedirs(report_path.parent, exist_ok=True)
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ“„ JSON æŠ¥å‘Š: {report_path}")


if __name__ == "__main__":
    main()
