"""
CBETA Bookcase XML â†’ æœç´¢ä¸“ç”¨ SQLite æ•°æ®åº“
ä½¿ç”¨ä¸é˜…è¯»é¡µé¢ç›¸åŒçš„åˆ†å·ç‰ˆ XML æ•°æ®ï¼ˆcbeta/XML/ï¼‰ï¼Œç”¨æˆ·åªéœ€ä¸‹è½½ä¸€ä»½æ•°æ®ã€‚

ç”¨æ³•ï¼š
    python etl_build_search.py --all               # è½¬æ¢å…¨éƒ¨ï¼ˆæ¨èï¼‰
    python etl_build_search.py --canon T            # è½¬æ¢å¤§æ­£è—
    python etl_build_search.py T0251                # è½¬æ¢å•éƒ¨ç»

æ•°æ®æºï¼šdata/raw/cbeta/XML/ï¼ˆBookcase åˆ†å·ç‰ˆï¼Œ21960 æ–‡ä»¶ï¼‰
è¾“å‡ºï¼š  data/db/cbeta_search.db
"""

import argparse
import glob
import json
import os
import re
import sqlite3
import sys
import time
import xml.etree.ElementTree as ET
from pathlib import Path

# æ·»åŠ æ¨¡å—æœç´¢è·¯å¾„
ETL_DIR = Path(__file__).resolve().parent
SRC_DIR = ETL_DIR.parent
sys.path.insert(0, str(ETL_DIR))
sys.path.insert(0, str(SRC_DIR))
import gaiji_map
import config

# OpenCC ç¹â†’ç®€
from opencc import OpenCC
cc_t2s = OpenCC('t2s')

# ============================================================
# é…ç½®ï¼ˆä» config æ¨¡å—è¯»å–ï¼Œé›¶ç¡¬ç¼–ç ï¼‰
# ============================================================

# Bookcase åˆ†å·ç‰ˆ XML è·¯å¾„ï¼ˆä¸é˜…è¯»é¡µé¢å…±ç”¨åŒä¸€ä»½æ•°æ®ï¼‰
XML_BASE = Path(os.getenv(
    "CBETA_XML_BASE",
    str(config.CBETA_BASE / "XML")
))

# æœç´¢æ•°æ®åº“è¾“å‡ºè·¯å¾„
DB_PATH = config.CBETA_SEARCH_DB

LOG_DIR = ETL_DIR / "logs"
GAIJI_PATH = config.GAIJI_PATH

# XML å‘½åç©ºé—´
TEI_NS = "http://www.tei-c.org/ns/1.0"
CB_NS = "http://www.cbeta.org/ns/1.0"
XML_NS = "http://www.w3.org/XML/1998/namespace"

ET.register_namespace("", TEI_NS)
ET.register_namespace("cb", CB_NS)

# ============================================================
# Schemaï¼ˆå†…åµŒï¼‰
# ============================================================
SCHEMA_SQL = """
CREATE TABLE IF NOT EXISTS catalog (
    sutra_id   TEXT PRIMARY KEY,
    canon      TEXT NOT NULL,
    title      TEXT,
    title_sc   TEXT,
    author     TEXT,
    total_juan INTEGER DEFAULT 1
);

CREATE TABLE IF NOT EXISTS content (
    id            INTEGER PRIMARY KEY AUTOINCREMENT,
    sutra_id      TEXT NOT NULL,
    juan          INTEGER NOT NULL,
    plain_text    TEXT,
    plain_text_sc TEXT,
    FOREIGN KEY (sutra_id) REFERENCES catalog(sutra_id),
    UNIQUE(sutra_id, juan)
);

CREATE VIRTUAL TABLE IF NOT EXISTS content_fts USING fts5(
    sutra_id,
    juan,
    plain_text_sc,
    content=content,
    content_rowid=id,
    tokenize='trigram'
);

CREATE TRIGGER IF NOT EXISTS content_ai AFTER INSERT ON content BEGIN
    INSERT INTO content_fts(rowid, sutra_id, juan, plain_text_sc)
    VALUES (new.id, new.sutra_id, new.juan, new.plain_text_sc);
END;

CREATE TRIGGER IF NOT EXISTS content_ad AFTER DELETE ON content BEGIN
    INSERT INTO content_fts(content_fts, rowid, sutra_id, juan, plain_text_sc)
    VALUES ('delete', old.id, old.sutra_id, old.juan, old.plain_text_sc);
END;

CREATE TRIGGER IF NOT EXISTS content_au AFTER UPDATE ON content BEGIN
    INSERT INTO content_fts(content_fts, rowid, sutra_id, juan, plain_text_sc)
    VALUES ('delete', old.id, old.sutra_id, old.juan, old.plain_text_sc);
    INSERT INTO content_fts(rowid, sutra_id, juan, plain_text_sc)
    VALUES (new.id, new.sutra_id, new.juan, new.plain_text_sc);
END;

CREATE INDEX IF NOT EXISTS idx_catalog_canon ON catalog(canon);
CREATE INDEX IF NOT EXISTS idx_content_sutra ON content(sutra_id);
CREATE INDEX IF NOT EXISTS idx_catalog_title_sc ON catalog(title_sc);
"""


def init_db(db_path):
    """åˆå§‹åŒ–æœç´¢æ•°æ®åº“"""
    os.makedirs(os.path.dirname(db_path), exist_ok=True)
    conn = sqlite3.connect(str(db_path))
    conn.execute("PRAGMA journal_mode=WAL")
    conn.execute("PRAGMA synchronous=NORMAL")
    conn.executescript(SCHEMA_SQL)
    conn.commit()
    return conn


# ============================================================
# è¾…åŠ©å‡½æ•°
# ============================================================
def _local_tag(element):
    """è·å–å…ƒç´ çš„æœ¬åœ°åï¼ˆå»é™¤å‘½åç©ºé—´ï¼‰"""
    tag = element.tag
    if "}" in tag:
        return tag.split("}", 1)[1]
    return tag


# è·³è¿‡ç±»æ ‡ç­¾
SKIP_TAGS_TEXT = {
    "note", "rdg", "anchor", "back",
    "mulu", "charDecl", "teiHeader",
}

SELF_CLOSING = {
    "lb", "pb", "milestone", "anchor", "space", "caesura",
}


def get_text_recursive(element):
    """é€’å½’æå–å…ƒç´ çš„çº¯æ–‡æœ¬å†…å®¹ï¼Œè¦†ç›– CBETA XML å…¨éƒ¨æ ‡ç­¾"""
    parts = []
    if element.text:
        parts.append(element.text)

    for child in element:
        tag = _local_tag(child)

        if tag == "g":
            ref = child.get("ref", "")
            cb_id = ref.lstrip("#")
            resolved = gaiji_map.resolve(cb_id)
            parts.append(resolved)
        elif tag == "lem":
            parts.append(get_text_recursive(child))
        elif tag == "app":
            parts.append(get_text_recursive(child))
        elif tag in SKIP_TAGS_TEXT:
            pass
        elif tag == "space":
            quantity = child.get("quantity", "1")
            try:
                n = int(quantity)
            except ValueError:
                n = 1
            parts.append("ã€€" * n)
        elif tag == "caesura":
            parts.append("ã€€")
        elif tag == "choice":
            parts.append(get_text_recursive(child))
        elif tag in ("sic", "orig"):
            pass
        elif tag in ("corr", "reg"):
            parts.append(get_text_recursive(child))
        elif tag in SELF_CLOSING:
            pass
        else:
            parts.append(get_text_recursive(child))

        if child.tail:
            parts.append(child.tail)

    return "".join(parts)


# ============================================================
# å…ƒæ•°æ®æå–
# ============================================================
def extract_metadata(tree):
    """ä» teiHeader æå–ç»æ–‡å…ƒæ•°æ®"""
    root = tree.getroot()
    xml_id = root.get(f"{{{XML_NS}}}id", "")

    match = re.match(r"([A-Z]+)(\d+)n([a-z]*)(\d+[a-z]?)", xml_id)
    if match:
        canon = match.group(1)
        sutra_no = match.group(3) + match.group(4)
        sutra_id = f"{canon}{sutra_no.zfill(4)}"
    else:
        canon = ""
        sutra_id = xml_id

    title = xml_id
    for title_elem in root.iter(f"{{{TEI_NS}}}title"):
        if (
            title_elem.get("level") == "m"
            and title_elem.get(f"{{{XML_NS}}}lang") == "zh-Hant"
        ):
            extracted = get_text_recursive(title_elem).strip()
            if extracted:
                title = extracted
            break

    author = ""
    author_elem = root.find(f".//{{{TEI_NS}}}titleStmt/{{{TEI_NS}}}author")
    if author_elem is not None and author_elem.text:
        author = author_elem.text.strip()

    total_juan = 1
    extent_elem = root.find(f".//{{{TEI_NS}}}extent")
    if extent_elem is not None and extent_elem.text:
        juan_match = re.search(r"(\d+)", extent_elem.text)
        if juan_match:
            total_juan = int(juan_match.group(1))

    return {
        "sutra_id": sutra_id,
        "canon": canon,
        "title": title,
        "title_sc": cc_t2s.convert(title),
        "author": author,
        "total_juan": total_juan,
    }


# ============================================================
# ä»æ–‡ä»¶åè§£æå·å·
# ============================================================
def parse_juan_from_filename(filename):
    """ä» Bookcase æ–‡ä»¶åè§£æå·å·ï¼Œå¦‚ T08n0251_001.xml â†’ 1"""
    m = re.search(r"_(\d+)\.xml$", filename)
    if m:
        return int(m.group(1))
    return 1


# ============================================================
# å•æ–‡ä»¶å¤„ç†ï¼ˆBookcase åˆ†å·ç‰ˆï¼šæ¯æ–‡ä»¶ = ä¸€å·ï¼‰
# ============================================================
_processed_sutras = set()

def process_file(xml_path, conn):
    """å¤„ç†å•ä¸ª Bookcase XML æ–‡ä»¶ï¼ˆä¸€å·ï¼‰ï¼Œå†™å…¥æœç´¢æ•°æ®åº“"""
    global _processed_sutras
    try:
        with open(str(xml_path), "r", encoding="utf-8") as f:
            content = f.read()
        tree = ET.ElementTree(ET.fromstring(content))

        meta = extract_metadata(tree)
        sutra_id = meta["sutra_id"]
        juan = parse_juan_from_filename(os.path.basename(xml_path))

        # é¦–æ¬¡é‡åˆ°æ­¤ç»ï¼Œå†™å…¥ catalog
        if sutra_id not in _processed_sutras:
            _processed_sutras.add(sutra_id)
            conn.execute(
                """INSERT OR REPLACE INTO catalog
                   (sutra_id, canon, title, title_sc, author, total_juan)
                   VALUES (?, ?, ?, ?, ?, ?)""",
                (sutra_id, meta["canon"], meta["title"],
                 meta["title_sc"], meta["author"], meta["total_juan"]),
            )

        # ç›´æ¥æå– body çº¯æ–‡æœ¬ï¼ˆæ¯æ–‡ä»¶å°±æ˜¯ä¸€å·ï¼Œä¸éœ€è¦ milestone åˆ†å·ï¼‰
        root = tree.getroot()
        body = root.find(f".//{{{TEI_NS}}}body")
        if body is not None:
            plain_text = get_text_recursive(body)
            plain_text_sc = cc_t2s.convert(plain_text)

            conn.execute(
                """INSERT OR REPLACE INTO content
                   (sutra_id, juan, plain_text, plain_text_sc)
                   VALUES (?, ?, ?, ?)""",
                (sutra_id, juan, plain_text, plain_text_sc),
            )

        return sutra_id, juan

    except Exception as e:
        conn.rollback()
        print(f"  âŒ å¤„ç†å¤±è´¥ {xml_path}: {e}")
        import traceback
        traceback.print_exc()
        return None


# ============================================================
# æ–‡ä»¶å‘ç°ï¼ˆé€‚é… Bookcase ç›®å½•ç»“æ„ï¼‰
# Bookcase ç»“æ„: XML/{Canon}/{CanonVol}/{CanonVol}n{No}_{Juan}.xml
# ä¾‹: XML/T/T08/T08n0251_001.xml
# ============================================================
def find_xml_files(target):
    """æ ¹æ®ç›®æ ‡å‚æ•°æ‰¾åˆ°è¦å¤„ç†çš„ XML æ–‡ä»¶åˆ—è¡¨"""
    if target == "--all":
        return sorted(glob.glob(str(XML_BASE / "*" / "*" / "*.xml")))

    # è—ç»ä»£ç ï¼ˆå¦‚ T, X, Aï¼‰
    canon_dir = XML_BASE / target
    if canon_dir.is_dir():
        return sorted(glob.glob(str(canon_dir / "*" / "*.xml")))

    # ç»å·ï¼ˆå¦‚ T0251 æˆ– T08n0251ï¼‰
    # å°è¯•åŒ¹é…æ‰€æœ‰å·
    match_short = re.match(r"([A-Z]+)(\d+)$", target)
    if match_short:
        canon = match_short.group(1)
        sutra_no = match_short.group(2)
        # æœç´¢æ‰€æœ‰å·å†Œç›®å½•
        pattern = str(XML_BASE / canon / "*" / f"*n{sutra_no}_*.xml")
        files = sorted(glob.glob(pattern))
        if files:
            return files
        # ä¹Ÿå°è¯•ä¸è¡¥é›¶
        pattern2 = str(XML_BASE / canon / "*" / f"*n{sutra_no.lstrip('0')}_*.xml")
        files = sorted(glob.glob(pattern2))
        if files:
            return files
        print(f"âŒ æ‰¾ä¸åˆ°ç»å· {target} çš„æ–‡ä»¶")
        return []

    print(f"âŒ æ— æ³•è¯†åˆ«ç›®æ ‡: {target}")
    return []


# ============================================================
# ä¸»ç¨‹åº
# ============================================================
def main():
    parser = argparse.ArgumentParser(
        description="CBETA Bookcase XML â†’ æœç´¢ä¸“ç”¨æ•°æ®åº“ï¼ˆå«ç®€ä½“åˆ— + FTS5ï¼‰"
    )
    parser.add_argument(
        "target", nargs="?", default=None,
        help="ç»å·ï¼ˆå¦‚ T0251ï¼‰æˆ–è—ç»ä»£ç ï¼ˆå¦‚ Tï¼‰",
    )
    parser.add_argument("--canon", type=str, help="æŒ‰è—ç»ä»£ç è½¬æ¢")
    parser.add_argument("--all", action="store_true", help="è½¬æ¢å…¨éƒ¨")
    args = parser.parse_args()

    if args.all:
        target = "--all"
    elif args.canon:
        target = args.canon
    elif args.target:
        target = args.target
    else:
        parser.print_help()
        return

    xml_files = find_xml_files(target)
    if not xml_files:
        return

    print(f"ğŸ“š æ‰¾åˆ° {len(xml_files)} ä¸ª XML æ–‡ä»¶å¾…è½¬æ¢")
    print(f"ğŸ“‚ æ•°æ®æº: {XML_BASE}")
    print(f"ğŸ’¾ æœç´¢æ•°æ®åº“: {DB_PATH}")
    print(f"ğŸ”¤ OpenCC ç¹â†’ç®€: å·²å¯ç”¨")
    print()

    global _processed_sutras
    _processed_sutras = set()

    conn = init_db(DB_PATH)
    gaiji_map.load_gaiji_map(str(GAIJI_PATH))
    print("âœ… Gaiji æ˜ å°„è¡¨å·²åŠ è½½")

    success = 0
    errors = []
    start_time = time.time()

    for i, xml_path in enumerate(xml_files, 1):
        filename = os.path.basename(xml_path)
        # æ¯100ä¸ªæ–‡ä»¶æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦ï¼Œé¿å…åˆ·å±
        if i % 100 == 1 or i == len(xml_files):
            print(f"  [{i}/{len(xml_files)}] {filename} ...", end=" ", flush=True)

        result = process_file(xml_path, conn)
        if result:
            if i % 100 == 1 or i == len(xml_files):
                sutra_id, juan = result
                print(f"âœ… {sutra_id} å·{juan}")
            success += 1
        else:
            errors.append(xml_path)

        # æ¯500ä¸ªæ–‡ä»¶æäº¤ä¸€æ¬¡
        if i % 500 == 0:
            conn.commit()

    conn.commit()
    elapsed = time.time() - start_time

    print()
    print("=" * 50)
    print(f"âœ… æˆåŠŸ: {success}/{len(xml_files)}")
    print(f"âŒ å¤±è´¥: {len(errors)}/{len(xml_files)}")
    print(f"â±ï¸ è€—æ—¶: {elapsed:.1f} ç§’")

    for table in ["catalog", "content"]:
        try:
            count = conn.execute(f"SELECT COUNT(*) FROM {table}").fetchone()[0]
            print(f"ğŸ“Š {table}: {count} æ¡")
        except Exception:
            pass

    db_size = os.path.getsize(str(DB_PATH))
    print(f"ğŸ’¾ æ•°æ®åº“ä½“ç§¯: {db_size / 1024 / 1024:.1f} MB")

    if errors:
        print()
        print(f"âŒ å¤±è´¥æ–‡ä»¶: {len(errors)} ä¸ª")
        os.makedirs(LOG_DIR, exist_ok=True)
        log_path = LOG_DIR / "search_etl_errors.log"
        with open(log_path, "w", encoding="utf-8") as f:
            for e in errors:
                f.write(f"{e}\n")
        print(f"  æ—¥å¿—å·²ä¿å­˜: {log_path}")

    conn.close()
    print(f"\nâœ… æœç´¢æ•°æ®åº“å·²ç”Ÿæˆ: {DB_PATH}")


if __name__ == "__main__":
    main()
